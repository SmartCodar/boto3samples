{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMF1BfbP4pJqizDaH7RLZSP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SmartCodar/boto3samples/blob/main/testfinetuned_llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbfXN_m03ug1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "p86XrUuk4K_1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "v0dTLH_14CuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"SmartCodar/ameetLlama-2-7b-chat-finetune\"\n",
        "prompt = \"What is a large language model?\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "ZtpVFl5h3z4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pipeline(\n",
        "    f'[INST] {prompt} [/INST]',\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=200,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVY8tt-j9pJ-",
        "outputId": "f5fb3cbd-bdfa-458b-ce7d-4a2e580a5471"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: [INST] What is a large language model? [/INST] A large language model (LLM) is a neural network-based model that is trained on vast amounts of text data to generate text that appears to be natural and similar to text written by humans.\n",
            "\n",
            "An LLM can be used for various tasks, such as writing articles, producing chatbot responses, or even creating entire books. They can be trained on different types of data, including news articles, books, or even social media posts. The goal of an LLM is to improve the quality and coherence of the text it generates, and to make it as natural-sounding as possible.\n",
            "\n",
            "The main difference between an LLM and a traditional language model is the amount of training data used. Traditional models may only be trained on a small dataset, while an LLM can be trained on a much larger dataset. This allows the LLM to learn more complex patterns and structures in language, making\n"
          ]
        }
      ]
    }
  ]
}